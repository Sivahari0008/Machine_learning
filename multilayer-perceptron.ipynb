{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n## Importing necessary modules\nimport tensorflow as tf\n#import numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-17T05:44:48.899038Z","iopub.execute_input":"2025-05-17T05:44:48.899453Z","iopub.status.idle":"2025-05-17T05:44:48.908130Z","shell.execute_reply.started":"2025-05-17T05:44:48.899412Z","shell.execute_reply":"2025-05-17T05:44:48.907088Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Load MNIST dataset\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n\n# Normalize image pixel values by dividing by 255 (grayscale)\ngray_scale = 255\n\nx_train = x_train.astype('float32') / gray_scale\nx_test = x_test.astype('float32') / gray_scale\n\n# Checking the shape of feature and target matrices\nprint(\"Feature matrix (x_train):\", x_train.shape)\nprint(\"Target matrix (y_train):\", y_train.shape)\nprint(\"Feature matrix (x_test):\", x_test.shape)\nprint(\"Target matrix (y_test):\", y_test.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualizing 100 images from the training data\nfig, ax = plt.subplots(10, 10)\nk = 0\nfor i in range(10):\n    for j in range(10):\n        ax[i][j].imshow(x_train[k].reshape(28, 28), aspect='auto', cmap='gray')\n        ax[i][j].axis('off')  # Hide axes for better visualization\n        k += 1\nplt.suptitle(\"Sample Images from MNIST Dataset\", fontsize=16)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Building the Sequential neural network model\nmodel = Sequential([\n    # Flatten input from 28x28 images to 784 (28*28) vector\n    Flatten(input_shape=(28, 28)),\n  \n    # Dense layer 1 (256 neurons)\n    Dense(256, activation='sigmoid'),  \n  \n    # Dense layer 2 (128 neurons)\n    Dense(128, activation='sigmoid'), \n  \n    # Output layer (10 classes)\n    Dense(10, activation='softmax'),  \n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compiling the model\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Training the model with training data\nhistory = model.fit(x_train, y_train, epochs=10, \n                    batch_size=2000, \n                    validation_split=0.2)\n\n# Evaluating the model on test data\nresults = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss, Test accuracy:', results)\n\n# Visualization of Training and Validation Accuracy/Loss\nplt.figure(figsize=(12, 5))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting Training and Validation Accuracy\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Training Accuracy', color='blue')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='orange')\nplt.title('Training and Validation Accuracy', fontsize=14)\nplt.xlabel('Epochs', fontsize=12)\nplt.ylabel('Accuracy', fontsize=12)\nplt.legend()\nplt.grid(True)\n\n# Plotting Training and Validation Loss\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Training Loss', color='blue')\nplt.plot(history.history['val_loss'], label='Validation Loss', color='orange')\nplt.title('Training and Validation Loss', fontsize=14)\nplt.xlabel('Epochs', fontsize=12)\nplt.ylabel('Loss', fontsize=12)\nplt.legend()\nplt.grid(True)plt.suptitle(\"Model Training Performance\", fontsize=16)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}